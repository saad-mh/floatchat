{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b27528fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: './datasets/'\n",
      "c:\\Personal Files\\Arya\\Programs\\Github\\float-chat\\floatchat\\backend\\datasets\n"
     ]
    }
   ],
   "source": [
    "%cd ./datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f097d1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manda\\AppData\\Local\\Temp\\ipykernel_14292\\3617846196.py:20: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  n_prof = ds.dims.get('N_PROF', 1)\n",
      "<frozen _collections_abc>:811: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "C:\\Users\\manda\\AppData\\Local\\Temp\\ipykernel_14292\\3617846196.py:57: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "  n_levels = ds.dims.get('N_LEVELS', 0)\n",
      "<frozen _collections_abc>:811: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Profiles: (331, 7)\n",
      "‚úÖ Measurements: (16755, 6)\n",
      "‚úÖ Vector records: 331\n",
      "\n",
      "üìÅ Saved:\n",
      "- profiles_metadata.csv\n",
      "- measurements_data.csv\n",
      "- vector_metadata.json (for vector DB ingestion)\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "def parse_argo_profile(nc_path: str):\n",
    "    \"\"\"\n",
    "    Parse ARGO NetCDF profile file into structured DataFrames.\n",
    "    \n",
    "    Returns:\n",
    "        profiles_df: DataFrame with profile metadata (one row per profile)\n",
    "        measurements_df: DataFrame with depth measurements (multiple rows per profile)\n",
    "    \"\"\"\n",
    "    # Load the NetCDF file\n",
    "    ds = xr.open_dataset(nc_path, decode_timedelta=False)\n",
    "    \n",
    "    # Extract profile metadata\n",
    "    profiles_data = []\n",
    "    n_prof = ds.dims.get('N_PROF', 1)\n",
    "    \n",
    "    for i in range(n_prof):\n",
    "        # Extract profile-level metadata\n",
    "        profile_data = {\n",
    "            'profile_id': f\"prof_{i:03d}\",\n",
    "            'float_id': str(ds['PLATFORM_NUMBER'].values[i] if 'PLATFORM_NUMBER' in ds else f\"float_{i}\"),\n",
    "            'cycle_number': int(ds['CYCLE_NUMBER'].values[i]) if 'CYCLE_NUMBER' in ds else i,\n",
    "            'lat': float(ds['LATITUDE'].values[i]) if 'LATITUDE' in ds else None,\n",
    "            'lon': float(ds['LONGITUDE'].values[i]) if 'LONGITUDE' in ds else None,\n",
    "        }\n",
    "        \n",
    "        # Handle date extraction\n",
    "        if 'JULD' in ds:\n",
    "            try:\n",
    "                # Convert Julian day to datetime\n",
    "                juld = ds['JULD'].values[i]\n",
    "                if not np.isnan(juld):\n",
    "                    # ARGO uses days since 1950-01-01\n",
    "                    ref_date = pd.to_datetime('1950-01-01')\n",
    "                    profile_data['observation_date'] = ref_date + pd.Timedelta(days=juld)\n",
    "                else:\n",
    "                    profile_data['observation_date'] = None\n",
    "            except:\n",
    "                profile_data['observation_date'] = None\n",
    "        else:\n",
    "            profile_data['observation_date'] = None\n",
    "            \n",
    "        # Add QC summary placeholder\n",
    "        profile_data['qc_summary'] = 'Data available'\n",
    "        \n",
    "        profiles_data.append(profile_data)\n",
    "    \n",
    "    profiles_df = pd.DataFrame(profiles_data)\n",
    "    \n",
    "    # Extract measurements data\n",
    "    measurements_data = []\n",
    "    n_levels = ds.dims.get('N_LEVELS', 0)\n",
    "    \n",
    "    for prof_idx in range(n_prof):\n",
    "        profile_id = f\"prof_{prof_idx:03d}\"\n",
    "        \n",
    "        for level_idx in range(n_levels):\n",
    "            # Extract depth measurements\n",
    "            measurement = {\n",
    "                'profile_id': profile_id,\n",
    "                'level': level_idx,\n",
    "            }\n",
    "            \n",
    "            # Extract depth (pressure)\n",
    "            if 'PRES' in ds:\n",
    "                pres = ds['PRES'].values[prof_idx, level_idx] if ds['PRES'].ndim > 1 else ds['PRES'].values[level_idx]\n",
    "                if not np.isnan(pres):\n",
    "                    measurement['depth'] = float(pres)  # Pressure as depth proxy\n",
    "                else:\n",
    "                    continue  # Skip invalid measurements\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            # Extract temperature\n",
    "            if 'TEMP' in ds:\n",
    "                temp = ds['TEMP'].values[prof_idx, level_idx] if ds['TEMP'].ndim > 1 else ds['TEMP'].values[level_idx]\n",
    "                measurement['temperature'] = float(temp) if not np.isnan(temp) else None\n",
    "                \n",
    "            # Extract salinity\n",
    "            if 'PSAL' in ds:\n",
    "                psal = ds['PSAL'].values[prof_idx, level_idx] if ds['PSAL'].ndim > 1 else ds['PSAL'].values[level_idx]\n",
    "                measurement['salinity'] = float(psal) if not np.isnan(psal) else None\n",
    "                \n",
    "            # Calculate density (approximate)\n",
    "            if measurement.get('temperature') and measurement.get('salinity'):\n",
    "                # Simple density calculation (rough approximation)\n",
    "                temp = measurement['temperature']\n",
    "                sal = measurement['salinity']\n",
    "                measurement['density'] = 1000 + (sal - 35) * 0.8 - (temp - 10) * 0.2\n",
    "            else:\n",
    "                measurement['density'] = None\n",
    "                \n",
    "            measurements_data.append(measurement)\n",
    "    \n",
    "    measurements_df = pd.DataFrame(measurements_data)\n",
    "    \n",
    "    # Clean up\n",
    "    ds.close()\n",
    "    \n",
    "    return profiles_df, measurements_df\n",
    "\n",
    "def parse_argo_profile_with_summary(nc_path: str):\n",
    "    \"\"\"\n",
    "    Parse ARGO NetCDF profile file into:\n",
    "      - profile metadata (profiles_df)\n",
    "      - depth measurements (measurements_df)\n",
    "      - vector DB summaries (vector_records)\n",
    "    \"\"\"\n",
    "    profiles_df, measurements_df = parse_argo_profile(nc_path)\n",
    "\n",
    "    vector_records = []\n",
    "\n",
    "    if not profiles_df.empty and not measurements_df.empty:\n",
    "        for profile_id in profiles_df[\"profile_id\"].unique():\n",
    "            subset = measurements_df[measurements_df[\"profile_id\"] == profile_id]\n",
    "\n",
    "            if subset.empty:\n",
    "                continue\n",
    "\n",
    "            # Compute derived stats\n",
    "            min_depth = subset[\"depth\"].min()\n",
    "            max_depth = subset[\"depth\"].max()\n",
    "            mean_temp = subset[\"temperature\"].mean()\n",
    "            mean_salinity = subset[\"salinity\"].mean()\n",
    "            mean_density = subset[\"density\"].mean() if \"density\" in subset else None\n",
    "\n",
    "            # Lookup profile metadata\n",
    "            meta = profiles_df[profiles_df[\"profile_id\"] == profile_id].iloc[0]\n",
    "\n",
    "            # Create summary text\n",
    "            summary_text = (\n",
    "                f\"Profile {profile_id} from float {meta['float_id']} observed on {meta['observation_date']} \"\n",
    "                f\"at location ({meta['lat']}, {meta['lon']}). \"\n",
    "                f\"Cycle number {meta['cycle_number']}. \"\n",
    "                f\"Depth range: {min_depth:.1f}m to {max_depth:.1f}m. \"\n",
    "                f\"Mean temperature: {mean_temp:.2f}¬∞C, mean salinity: {mean_salinity:.2f} PSU, \"\n",
    "                f\"mean density: {mean_density:.2f} kg/m¬≥. \"\n",
    "                f\"QC summary: {meta['qc_summary']}.\"\n",
    "            )\n",
    "\n",
    "            # Create vector DB record\n",
    "            vector_record = {\n",
    "                \"id\": profile_id,\n",
    "                \"text\": summary_text,\n",
    "                \"metadata\": {\n",
    "                    \"float_id\": meta[\"float_id\"],\n",
    "                    \"cycle_number\": int(meta[\"cycle_number\"]) if pd.notna(meta[\"cycle_number\"]) else None,\n",
    "                    \"date\": str(meta[\"observation_date\"]) if pd.notna(meta[\"observation_date\"]) else None,\n",
    "                    \"lat\": float(meta[\"lat\"]) if pd.notna(meta[\"lat\"]) else None,\n",
    "                    \"lon\": float(meta[\"lon\"]) if pd.notna(meta[\"lon\"]) else None,\n",
    "                    \"min_depth\": float(min_depth),\n",
    "                    \"max_depth\": float(max_depth),\n",
    "                    \"mean_temp\": float(mean_temp) if pd.notna(mean_temp) else None,\n",
    "                    \"mean_salinity\": float(mean_salinity) if pd.notna(mean_salinity) else None,\n",
    "                    \"mean_density\": float(mean_density) if pd.notna(mean_density) else None,\n",
    "                }\n",
    "            }\n",
    "\n",
    "            vector_records.append(vector_record)\n",
    "\n",
    "    return profiles_df, measurements_df, vector_records\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Example usage\n",
    "# -------------------------------\n",
    "nc_file = \"1900054_prof.nc\"  # replace with your file\n",
    "\n",
    "profiles_df, measurements_df, vector_records = parse_argo_profile_with_summary(nc_file)\n",
    "\n",
    "print(f\"‚úÖ Profiles: {profiles_df.shape}\")\n",
    "print(f\"‚úÖ Measurements: {measurements_df.shape}\")\n",
    "print(f\"‚úÖ Vector records: {len(vector_records)}\")\n",
    "\n",
    "# Save outputs\n",
    "profiles_df.to_csv(\"profiles_metadata.csv\", index=False)\n",
    "measurements_df.to_csv(\"measurements_data.csv\", index=False)\n",
    "with open(\"vector_metadata.json\", \"w\") as f:\n",
    "    json.dump(vector_records, f, indent=2)\n",
    "\n",
    "print(\"\\nüìÅ Saved:\")\n",
    "print(\"- profiles_metadata.csv\")\n",
    "print(\"- measurements_data.csv\")\n",
    "print(\"- vector_metadata.json (for vector DB ingestion)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
