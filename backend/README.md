# NetCDF Argo Pipeline ‚Äî End-to-End Setup Guide

This guide takes you from zero to a working, menu-driven NetCDF ingestion pipeline that stores Argo float data in Supabase and indexes metadata in Upstash Vector.

## 1) Create a Python virtual environment

```bash
# Linux/macOS
python3 -m venv .venv
source .venv/bin/activate

# Windows (PowerShell)
python -m venv .venv
.venv\Scripts\Activate.ps1
```

Upgrade pip:
```bash
python -m pip install --upgrade pip
```


## 2) System packages (recommended)

Some libraries need native dependencies.

**Linux (Debian/Ubuntu):**
```bash
sudo apt-get update
sudo apt-get install -y build-essential python3-dev \
     libhdf5-dev libnetcdf-dev graphviz
```

**macOS (Homebrew):**
```bash
brew install hdf5 netcdf graphviz
```

## 3) Create requirements.txt

Create a file named `requirements.txt` with the following content:

```txt
requests
beautifulsoup4
tqdm
loguru
xarray
netCDF4
h5netcdf
pydap
fsspec
jinja2
aiohttp
numpy
pandas
matplotlib
chromadb
sentence-transformers
psycopg2-binary
sqlalchemy
sqlalchemy_schemadisplay
pydot
graphviz
upstash-vector
supabase
python-dotenv
```


## 4) Project structure

Suggested structure:

```
your-project/
  .env
  requirements.txt
  main.py
  netcdf_test.py
  incois_scraper.py
  generate_erd.py
  valid_profiles.json        # generated by menu option 1
```

You already have `main.py` and `netcdf_test.py` from previous steps. Keep them as is.

## 5) Create Supabase and Upstash accounts

### Supabase

1. Create a new project in Supabase.
2. In **Project Settings ‚Üí API**, copy:
   - **Project URL** (this is your `SUPABASE_URL`, typically ends with `.supabase.co`).
   - **anon public API key** (use as `SUPABASE_KEY` for this pipeline).
3. Go to **SQL Editor** (we will run the schema in the next step).

### Upstash Vector

1. Create a Vector index.
2. Copy:
   - **REST URL**
   - **REST Token**


## 6) Create .env

Create a file named `.env` in the project root:

```env
# Supabase
SUPABASE_URL=https://YOUR-PROJECT-REF.supabase.co
SUPABASE_KEY=YOUR_SUPABASE_ANON_OR_SERVICE_KEY

# Upstash Vector
UPSTASH_VECTOR_REST_URL=https://YOUR-UPSTASH-URL
UPSTASH_VECTOR_REST_TOKEN=YOUR-UPSTASH-TOKEN
```

**Notes:**
- Keep keys private.
- The code uses anon key. If you use the service role key, treat it securely.

## 7) Initialize the database schema in Supabase

Open the Supabase SQL Editor and run this entire script:

```sql

-- ===============================
-- üö® Clean reset (drops old tables)
-- ===============================
DROP TABLE IF EXISTS float_locations;
DROP TABLE IF EXISTS float_measurements;
DROP TABLE IF EXISTS float_profiles;
DROP TABLE IF EXISTS floats;

-- ===============================
-- üß≠ Table 1: floats
-- ===============================
CREATE TABLE floats (
  float_id TEXT PRIMARY KEY,
  meta JSONB,
  created_at TIMESTAMP WITHOUT TIME ZONE DEFAULT NOW()
);

-- ===============================
-- üìã Table 2: float_profiles
-- ===============================
CREATE TABLE float_profiles (
  float_id TEXT NOT NULL,
  cycle_number TEXT NOT NULL,
  data_mode TEXT,
  date_creation TEXT,
  date_update TEXT,
  data_type TEXT,
  format_version TEXT,
  handbook_version TEXT,
  reference_date_time TEXT,
  platform_number TEXT,
  project_name TEXT,
  pi_name TEXT,
  direction TEXT,
  PRIMARY KEY (float_id, cycle_number),
  FOREIGN KEY (float_id) REFERENCES floats (float_id) ON DELETE CASCADE
);

-- ===============================
-- üåä Table 3: float_measurements
-- ===============================
CREATE TABLE float_measurements (
  measurement_id TEXT PRIMARY KEY,
  float_id TEXT NOT NULL,
  cycle_number TEXT,
  pres_adjusted DOUBLE PRECISION,
  temp_adjusted DOUBLE PRECISION,
  psal_adjusted DOUBLE PRECISION,
  FOREIGN KEY (float_id) REFERENCES floats (float_id) ON DELETE CASCADE
);

-- ===============================
-- üìç Table 4: float_locations
-- ===============================
CREATE TABLE float_locations (
  measurement_id TEXT PRIMARY KEY,
  float_id TEXT NOT NULL,
  cycle_number TEXT,
  juld DOUBLE PRECISION,
  latitude DOUBLE PRECISION,
  longitude DOUBLE PRECISION,
  FOREIGN KEY (float_id) REFERENCES floats (float_id) ON DELETE CASCADE
);

-- ===============================
-- ‚ö° Indexes for performance
-- ===============================
CREATE INDEX idx_float_profiles_float_id ON float_profiles (float_id);
CREATE INDEX idx_float_measurements_float_id ON float_measurements (float_id);
CREATE INDEX idx_float_measurements_cycle_number ON float_measurements (cycle_number);
CREATE INDEX idx_float_locations_float_id ON float_locations (float_id);
CREATE INDEX idx_float_locations_cycle_number ON float_locations (cycle_number);
CREATE INDEX idx_float_locations_lat_lon ON float_locations (latitude, longitude);

-- ===============================
-- ‚úÖ Verification message
-- ===============================
SELECT '‚úÖ All tables recreated successfully with normalized schema.' AS status;
```

Wait for success.

## 8) Prepare valid_profiles.json

Use the interactive menu in `main.py` to generate the list of Argo profile directories from INCOIS. Option 1 creates `valid_profiles.json` with entries like:

```json
[
  { "id": "1900121", "url": "https://data-argo.ifremer.fr/dac/incois/1900121/", "doMetaExist": true },
  ...
]
```

## 9) Run the menu-driven pipeline

From your project root:
```bash
python -m main
```


You will see a menu similar to:
```

============================================================
NetCDF Argo Pipeline ‚Äî Team Picasso (Supabase + Upstash)
============================================================
Recommended order: 1 ‚Üí 2 ‚Üí 3 ‚Üí 4
Choose 5 ONLY if you know what you‚Äôre doing!

1) Generate profile URLs (valid_profiles.json)
2) Test Supabase Connection
3) Fetch NetCDF files, normalize, and store in Supabase + Upstash
4) Create an ER Diagram
5) Purge Database (WARNING!! DESTRUCTIVE)
6) Exit
```

**Typical flow:**
1. **Option 1** ‚Üí generate `valid_profiles.json` (first time).
2. **Option 2** ‚Üí quick health check.
3. **Option 3** ‚Üí ingest NetCDF profiles; writes to `floats`, `float_profiles`, `float_measurements`, `float_locations`, and indexes metadata in Upstash.
4. **Option 4** ‚Üí optional ERD generation (requires Graphviz installed).

## 10) Notes and troubleshooting

- If you change table schemas, rerun the SQL script to reset, then re-ingest.
- If you see JSON/NaN errors, the code already normalizes `NaN`/`Inf` to `NULL`.
- `float_locations` is separate to avoid sparse columns in the main measurements table.
- `juld` is stored as a numeric (Julian date). Convert to timestamps in queries if needed.
- Make sure `.env` values match your Supabase and Upstash credentials exactly.
- For large ingestions, you can lower the threshold in the prompt when running Option 3.
